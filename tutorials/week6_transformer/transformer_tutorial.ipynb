{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8fccf2d",
      "metadata": {
        "id": "e8fccf2d"
      },
      "source": [
        "# Transformer Tutorial â€” From Scratch then PyTorch `nn.Transformer`\n",
        "\n",
        "**Format:** formal tutorial, runnable.  \n",
        "**What you'll learn (high-level):**\n",
        "\n",
        "1. Implement **scaled dot-product self-attention** from scratch.  \n",
        "2. Build **multi-head attention** and verify against `nn.MultiheadAttention`.  \n",
        "3. Implement a **Transformer encoder block** with residuals, layer-norm, and feed-forward.  \n",
        "4. Add **causal (masked) attention** and construct a simple Transformer decoder for autoregressive prediction.  \n",
        "5. Compare a **self-defined Transformer** with PyTorch's **packed** `nn.Transformer` on identical inputs (numerical closeness checks).  \n",
        "6. Train both models on a small dataset (Tiny Shakespeare) and compare loss/accuracy.\n",
        "\n",
        "Each exercise is presented as code cells followed by verification checks students can run to confirm correctness. The notebook is self-contained and executable locally (it will download the dataset from the web at runtime).\n",
        "\n",
        "**Notes for instructors / students:** run cells sequentially. If you want faster runs, reduce `d_model`, `num_heads`, sequence lengths, and training epochs in the training section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3c9ea6b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c9ea6b5",
        "outputId": "18a62d32-7888-4e10-928f-110de42d9ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and helper utilities\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('Device:', 'cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea50e1da",
      "metadata": {
        "id": "ea50e1da"
      },
      "source": [
        "## 1) Scaled Dot-Product Self-Attention (from scratch)\n",
        "\n",
        "We will implement the core attention computation:\n",
        "\n",
        "$\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "We'll provide a PyTorch `nn.Module` that:\n",
        "- projects input `x` to queries, keys, values with `nn.Linear`\n",
        "- computes attention scores, softmax, and returns output and attention weights\n",
        "\n",
        "This is expected to operate on input shape `(batch, seq_len, d_model)` and return the same output shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6866ee",
      "metadata": {
        "id": "8f6866ee"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key   = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        # scale factor uses sqrt(d_k)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: [B, T, D]\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        # scores: [B, T, T]\n",
        "        scores = torch.bmm(Q, K.transpose(1,2)) / self.scale\n",
        "        if mask is not None:\n",
        "            # mask expected to contain -inf for blocked positions (or very negative)\n",
        "            scores = scores + mask\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        out = torch.bmm(weights, V)\n",
        "        return out, weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bb7197",
      "metadata": {
        "id": "75bb7197"
      },
      "source": [
        "### Verification\n",
        "Check that attention weights sum to 1 across the last dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebba253",
      "metadata": {
        "id": "7ebba253"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Quick unit test\n",
        "B, T, D = 2, 4, 8\n",
        "x = torch.randn(B, T, D)\n",
        "att = SelfAttention(D)\n",
        "out, w = att(x)\n",
        "print('out.shape =', out.shape)\n",
        "print('weights.shape =', w.shape)\n",
        "print('row sums (first example):', w[0].sum(dim=-1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5114a00",
      "metadata": {
        "id": "d5114a00"
      },
      "source": [
        "## 2) Multi-Head Attention\n",
        "\n",
        "Multi-head attention runs several attention 'heads' in parallel, each with smaller `d_k = d_model/num_heads`, then concatenates the outputs and applies a final linear projection.\n",
        "\n",
        "We'll implement `MultiHeadSelfAttention` and then compare its output with `torch.nn.MultiheadAttention` (after reshaping inputs appropriately).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d3c6768",
      "metadata": {
        "id": "4d3c6768"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        # use single large linear and reshape trick for simplicity\n",
        "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: [B, T, D]\n",
        "        B, T, D = x.shape\n",
        "        qkv = self.qkv(x)  # [B, T, 3D]\n",
        "        qkv = qkv.view(B, T, 3, self.num_heads, self.d_k)  # [B, T, 3, H, d_k]\n",
        "        q, k, v = qkv[:,:,0], qkv[:,:,1], qkv[:,:,2]  # each [B, T, H, d_k]\n",
        "        # transpose to [B, H, T, d_k]\n",
        "        q = q.permute(0,2,1,3)\n",
        "        k = k.permute(0,2,1,3)\n",
        "        v = v.permute(0,2,1,3)\n",
        "        # compute scaled dot-product for each head\n",
        "        # reshape for batched matmul: combine B and H\n",
        "        q = q.contiguous().view(B*self.num_heads, T, self.d_k)\n",
        "        k = k.contiguous().view(B*self.num_heads, T, self.d_k)\n",
        "        v = v.contiguous().view(B*self.num_heads, T, self.d_k)\n",
        "        scores = torch.bmm(q, k.transpose(1,2)) / self.scale  # [B*H, T, T]\n",
        "        if mask is not None:\n",
        "            # mask must be broadcastable to [B*H, T, T]\n",
        "            scores = scores + mask\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        out = torch.bmm(weights, v)  # [B*H, T, d_k]\n",
        "        out = out.view(B, self.num_heads, T, self.d_k).permute(0,2,1,3).contiguous()\n",
        "        out = out.view(B, T, D)\n",
        "        out = self.out_proj(out)\n",
        "        return out, weights.view(B, self.num_heads, T, T)\n",
        "\n",
        "# Simple smoke test\n",
        "B, T, D = 2, 5, 12\n",
        "x = torch.randn(B, T, D)\n",
        "mha = MultiHeadSelfAttention(D, num_heads=3)\n",
        "out, w = mha(x)\n",
        "print('out.shape', out.shape)\n",
        "print('weights.shape', w.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d66a7e",
      "metadata": {
        "id": "d2d66a7e"
      },
      "source": [
        "## 3) Transformer Encoder Block\n",
        "\n",
        "An encoder block consists of:\n",
        "- Multi-head self-attention + residual + layer-norm\n",
        "- Position-wise feed-forward network (two linear layers with ReLU) + residual + layer-norm\n",
        "\n",
        "We'll implement `TransformerEncoderLayerSimple` inspired by the original paper but in an educational, explicit form.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f995a1b2",
      "metadata": {
        "id": "f995a1b2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerEncoderLayerSimple(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention block\n",
        "        attn_out, _ = self.mha(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.ln1(x)\n",
        "        # Feed-forward block\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "# smoke test\n",
        "B, T, D = 2, 6, 12\n",
        "enc = TransformerEncoderLayerSimple(D, num_heads=3, d_ff=48)\n",
        "x = torch.randn(B, T, D)\n",
        "out = enc(x)\n",
        "print('out.shape', out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc935463",
      "metadata": {
        "id": "bc935463"
      },
      "source": [
        "## 4) Positional Encoding & Small Transformer (Encoder or Decoder)\n",
        "\n",
        "Transformers are position-agnostic; we add sinusoidal positional encodings (or learned embeddings). We'll implement sinusoidal positional encodings and a small model that stacks multiple encoder layers.\n",
        "\n",
        "For each position $pos$ (e.g., 0, 1, 2, 3, â€¦) and each dimension $i$ of the modelâ€™s hidden size $d_{model}$, we define the sinusoidal positional encoding as:\n",
        "\n",
        "$\\mathrm{PE}_{(pos,\\,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}\\right)$\n",
        "\n",
        "\n",
        "$\\mathrm{PE}_{(pos,\\,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}\\right)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737d304a",
      "metadata": {
        "id": "737d304a"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SimpleTransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, d_ff=512, num_layers=2, max_len=512):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayerSimple(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, mask=None):\n",
        "        # idx: [B, T] token indices\n",
        "        x = self.tok_emb(idx)\n",
        "        x = self.pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        x = self.ln(x)\n",
        "        logits = self.output_proj(x)\n",
        "        return logits\n",
        "\n",
        "# smoke test with tiny vocab\n",
        "vocab_size = 50\n",
        "model = SimpleTransformerEncoder(vocab_size, d_model=64, num_heads=4, d_ff=128, num_layers=2)\n",
        "B, T = 2, 10\n",
        "idx = torch.randint(0, vocab_size, (B, T))\n",
        "logits = model(idx)\n",
        "print('logits.shape', logits.shape)  # [B, T, vocab_size]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97ee374",
      "metadata": {
        "id": "b97ee374"
      },
      "source": [
        "## 5) Tiny Shakespeare dataset & training utilities\n",
        "\n",
        "We'll use Karpathy's Tiny Shakespeare (~1 MB) for a character-level language modeling task (next-character prediction). The notebook will download the raw text at runtime. If running offline, place the file at `data/tiny_shakespeare.txt`.\n",
        "\n",
        "We'll create a small `Dataset` to produce fixed-length sequences and targets for next-token prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66ef595",
      "metadata": {
        "id": "a66ef595"
      },
      "outputs": [],
      "source": [
        "# Download dataset (if not present)\n",
        "data_dir = Path('data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "local_path = data_dir / 'tiny_shakespeare.txt'\n",
        "if not local_path.exists():\n",
        "    print('Downloading Tiny Shakespeare...')\n",
        "    urllib.request.urlretrieve(url, local_path)\n",
        "else:\n",
        "    print('Dataset already exists at', local_path)\n",
        "\n",
        "text = local_path.read_text()\n",
        "print('dataset length (chars):', len(text))\n",
        "\n",
        "# build vocab\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print('vocab size:', vocab_size)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "\n",
        "# dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_len=100):\n",
        "        self.seq_len = seq_len\n",
        "        self.data = data\n",
        "        self.ids = [stoi[c] for c in data]\n",
        "    def __len__(self):\n",
        "        return 5000\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.ids[idx:idx+self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.ids[idx+1:idx+self.seq_len+1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# quick smoke\n",
        "ds = CharDataset(text, seq_len=64)\n",
        "print('dataset len', len(ds))\n",
        "print('example shapes', ds[0][0].shape, ds[0][1].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7527bd44",
      "metadata": {
        "id": "7527bd44"
      },
      "source": [
        "## 6) Training both models and comparing performance\n",
        "\n",
        "We'll:\n",
        "- Instantiate **our** `SimpleTransformerEncoder` and a **packed** `nn.Transformer` style model (using `nn.TransformerEncoder`).\n",
        "- Train for a small number of steps/epochs to demonstrate training behavior.\n",
        "\n",
        "**Warning:** full training can be slow. The provided defaults are small so the notebook runs in reasonable time on CPU. Increase model sizes only if you have GPU and time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451ff415",
      "metadata": {
        "id": "451ff415"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "# -----------------------------\n",
        "# Training & evaluation utilities\n",
        "# -----------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for xb, yb in tqdm(dataloader):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)  # [B, T, V]\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(dataloader):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            B, T, V = logits.shape\n",
        "            loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Setup device, data, model\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seq_len = 64\n",
        "batch_size = 16\n",
        "\n",
        "# Dataloaders\n",
        "full_dataset = CharDataset(text, seq_len=seq_len)\n",
        "train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(full_dataset, batch_size=batch_size)\n",
        "\n",
        "# Our custom Transformer\n",
        "model = SimpleTransformerEncoder(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=64,\n",
        "    num_heads=4,\n",
        "    d_ff=256,\n",
        "    num_layers=2,\n",
        "    max_len=512\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop with loss recording\n",
        "# -----------------------------\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    t1 = time.time()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | time {t1-t0:.1f}s | \"\n",
        "        f\"train loss {train_loss:.4f} | val loss {val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612666aa",
      "metadata": {
        "id": "612666aa"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Plot losses\n",
        "# -----------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, epochs+1), train_losses, marker='o', label='Train Loss')\n",
        "plt.plot(range(1, epochs+1), val_losses, marker='o', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ejoSCy2NnS"
      },
      "source": [
        "# 7) Decoder and Masking\n",
        "\n",
        "This section fills in the missing **decoder** and **masking** pieces for our mini-Transformer tutorial.\n",
        "\n",
        "**Goals**\n",
        "- Understand *why* we need **masked selfâ€‘attention** (a.k.a. lookâ€‘ahead/causal mask) in the decoder.\n",
        "- Build a clean **`DecoderLayer`** with both selfâ€‘attention and crossâ€‘attention.\n",
        "- Implement practical **mask utilities**: subsequent mask (causal) and padding masks.\n",
        "- Verify masks via a small **visualization demo**.\n",
        "- Practice with short **exercises** (and reference answers)."
      ],
      "id": "26ejoSCy2NnS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud-oCAjS2NnS"
      },
      "source": [
        "## 7.1 Why masking in the decoder?\n",
        "\n",
        "During sequence generation, the decoder predicts token *t* using only tokens **â‰¤ t**. To enforce this, we apply a **lookâ€‘ahead (causal) mask** so position *t* cannot attend to positions *> t*.  \n",
        "We also apply **padding masks** to ignore `<pad>` tokens when sequences have different lengths.\n",
        "\n",
        "Let $Q,K,V \\in \\mathbb{R}^{T \\times d}$. Scaled dotâ€‘product attention is\n",
        "$\\mathrm{softmax}((QK^\\top)/\\sqrt{d_k} + M)V,$\n",
        "where $M_{ij}=-\\infty$ if position $i$ is **not allowed** to see $j$ (future tokens or pads), and 0 otherwise."
      ],
      "id": "Ud-oCAjS2NnS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClGymty82NnT"
      },
      "source": [
        "## 7.2 Mask utilities\n",
        "\n",
        "We implement:\n",
        "- **`generate_square_subsequent_mask(L)`** â†’ a boolean matrix shaped `[L, L]` that masks *future* tokens (upper triangular).\n",
        "- **`make_padding_mask(tokens, pad_idx=0)`** â†’ a boolean matrix shaped `[B, L]` that masks pad positions per batch."
      ],
      "id": "ClGymty82NnT"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AwlkhX982NnT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_square_subsequent_mask(L: int, device=None):\n",
        "    \"\"\"\n",
        "    Causal (look-ahead) mask for decoder self-attention.\n",
        "    Returns a [L, L] boolean tensor where True means \"mask out\".\n",
        "    \"\"\"\n",
        "    m = torch.triu(torch.ones((L, L), device=device, dtype=torch.bool), diagonal=1)\n",
        "    return m  # True above the diagonal -> disallow attending to future\n",
        "\n",
        "def make_padding_mask(tokens: torch.Tensor, pad_idx: int = 0):\n",
        "    \"\"\"\n",
        "    tokens: LongTensor [B, L]\n",
        "    Returns: BoolTensor [B, L], True for pad positions.\n",
        "    \"\"\"\n",
        "    return tokens.eq(pad_idx)"
      ],
      "id": "AwlkhX982NnT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYbGPEg2NnU"
      },
      "source": [
        "## 7.3 A minimal `DecoderLayer`\n",
        "\n",
        "This layer stacks:\n",
        "1) **Masked selfâ€‘attention** over the target sequence  \n",
        "2) **Crossâ€‘attention** that queries the encoder output (\"memory\")  \n",
        "3) A positionâ€‘wise feedâ€‘forward network  \n",
        "\n",
        "Each sublayer has residual + layer norm."
      ],
      "id": "6gYbGPEg2NnU"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DZh2CXuJ2NnU"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,                       # [B, Lt, d]\n",
        "        memory,                    # [B, Ls, d] encoder outputs\n",
        "        tgt_mask=None,             # [Lt, Lt]   (causal)\n",
        "        tgt_key_padding_mask=None, # [B, Lt]\n",
        "        memory_key_padding_mask=None # [B, Ls]\n",
        "    ):\n",
        "        # 1) masked self-attention\n",
        "        x = tgt\n",
        "        _sa_out, _ = self.self_attn(\n",
        "            x, x, x,\n",
        "            attn_mask=tgt_mask,\n",
        "            key_padding_mask=tgt_key_padding_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "        x = x + self.dropout1(_sa_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # 2) cross-attention\n",
        "        _ca_out, _ = self.cross_attn(\n",
        "            x, memory, memory,\n",
        "            key_padding_mask=memory_key_padding_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "        x = x + self.dropout2(_ca_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # 3) feed-forward\n",
        "        _ff = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        x = x + self.dropout3(_ff)\n",
        "        x = self.norm3(x)\n",
        "        return x"
      ],
      "id": "DZh2CXuJ2NnU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_qFW6q82NnV"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int, num_layers: int, dim_feedforward: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        x = tgt\n",
        "        for layer in self.layers:\n",
        "            x = layer(\n",
        "                x, memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "        return self.norm(x)"
      ],
      "id": "y_qFW6q82NnV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_3PMZmk2NnW"
      },
      "source": [
        "## 7.4 Demo: Visualize the causal mask\n",
        "\n",
        "We show the boolean **subsequent mask** and verify attention cannot see future tokens."
      ],
      "id": "T_3PMZmk2NnW"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "m6mtIy4J2NnW",
        "outputId": "43dbb49e-ee65-472f-8e90-ab2a323801be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "L = 10\n",
        "mask = generate_square_subsequent_mask(L)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mask.cpu().numpy(), interpolation='nearest')\n",
        "plt.title(\"Causal (look-ahead) mask: True=blocked\")\n",
        "plt.xlabel(\"Key positions j\")\n",
        "plt.ylabel(\"Query positions i\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "m6mtIy4J2NnW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrY_JI0Z2NnX"
      },
      "source": [
        "### Exercise 1 (masking)\n",
        "\n",
        "Implement a function `apply_causal_mask(scores)` that sets the *upper triangle* of the `[L, L]` attention score matrix to `-inf` so softmax will zero it out.\n",
        "\n",
        "**Hint:** use `torch.triu` with `diagonal=1`.\n",
        "\n",
        "*(Write your solution in the next cell. An answer is provided after it.)*"
      ],
      "id": "FrY_JI0Z2NnX"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "REbJjdGN2NnX"
      },
      "outputs": [],
      "source": [
        "# TODO: Your implementation here\n",
        "import torch\n",
        "\n",
        "def apply_causal_mask(scores: torch.Tensor):\n",
        "    \"\"\"\n",
        "    scores: [L, L] float tensor (single head for simplicity)\n",
        "    Return: masked scores\n",
        "    \"\"\"\n",
        "    L = scores.size(-1)\n",
        "    # --- your code ---\n",
        "    raise NotImplementedError(\"Fill the causal mask here\")"
      ],
      "id": "REbJjdGN2NnX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocjzEpZK2NnY"
      },
      "source": [
        "**Answer (Exercise 1):**"
      ],
      "id": "ocjzEpZK2NnY"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wU3Le-C2NnY",
        "outputId": "cf50c33b-0636-4b8e-9ad8-acb8cbba7848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upper triangle is -inf: True\n"
          ]
        }
      ],
      "source": [
        "def apply_causal_mask(scores: torch.Tensor):\n",
        "    L = scores.size(-1)\n",
        "    mask = torch.triu(torch.ones(L, L, dtype=torch.bool, device=scores.device), diagonal=1)\n",
        "    scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "    return scores\n",
        "\n",
        "# quick test\n",
        "torch.manual_seed(0)\n",
        "S = torch.randn(5,5)\n",
        "Sm = apply_causal_mask(S.clone())\n",
        "print(\"Upper triangle is -inf:\", torch.isinf(Sm).sum().item() == (5*4)//2)"
      ],
      "id": "4wU3Le-C2NnY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCBteFNK2NnZ"
      },
      "source": [
        "### Exercise 2 (decoder step)\n",
        "\n",
        "Given `tgt` (decoder inputs) and `memory` (encoder outputs), run one **`DecoderLayer`** with a correct **`tgt_mask`** from `generate_square_subsequent_mask`.\n",
        "\n",
        "*(Write your solution in the next cell. An answer is provided after it.)*"
      ],
      "id": "gCBteFNK2NnZ"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HEuoIQoi2NnZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Your implementation here\n",
        "B, Lt, Ls, d_model, nhead = 2, 7, 9, 32, 4\n",
        "torch.manual_seed(0)\n",
        "tgt = torch.randn(B, Lt, d_model)\n",
        "memory = torch.randn(B, Ls, d_model)\n",
        "\n",
        "layer = DecoderLayer(d_model, nhead, dim_feedforward=64, dropout=0.0)\n",
        "\n",
        "# --- your code ---\n",
        "# x = layer(...)\n",
        "# print(x.shape)  # expect [B, Lt, d_model]"
      ],
      "id": "HEuoIQoi2NnZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1hy9bFN2Nna"
      },
      "source": [
        "**Answer (Exercise 2):**"
      ],
      "id": "n1hy9bFN2Nna"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlYCmv8-2Nna",
        "outputId": "7c2f2253-d278-4a4a-cf50-a1a168b76ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 32])\n"
          ]
        }
      ],
      "source": [
        "B, Lt, Ls, d_model, nhead = 2, 7, 9, 32, 4\n",
        "torch.manual_seed(0)\n",
        "tgt = torch.randn(B, Lt, d_model)\n",
        "memory = torch.randn(B, Ls, d_model)\n",
        "\n",
        "layer = DecoderLayer(d_model, nhead, dim_feedforward=64, dropout=0.0)\n",
        "tgt_mask = generate_square_subsequent_mask(Lt)\n",
        "\n",
        "x = layer(tgt, memory, tgt_mask=tgt_mask)\n",
        "print(x.shape)"
      ],
      "id": "qlYCmv8-2Nna"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrlnraTc2Nnb"
      },
      "source": [
        "## 7.6 How to plug into your existing encoder\n",
        "\n",
        "If you already built an **`Encoder`** in previous sections, you can integrate as follows:\n",
        "\n",
        "```python\n",
        "# Encode source tokens\n",
        "src_emb = src_embed(src_tokens) + pos_embed_src  # [B, Ls, d]\n",
        "memory = encoder(src_emb, src_key_padding_mask=src_pad_mask)\n",
        "\n",
        "# Prepare decoder inputs (shifted right with <bos>), add positions\n",
        "tgt_inp = tgt_embed(tgt_tokens_in) + pos_embed_tgt  # [B, Lt, d]\n",
        "\n",
        "# Masks\n",
        "tgt_mask = generate_square_subsequent_mask(Lt)\n",
        "tgt_pad_mask = make_padding_mask(tgt_tokens_in, pad_idx=PAD)\n",
        "\n",
        "# Decode\n",
        "dec_out = decoder(tgt_inp, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask,\n",
        "                  memory_key_padding_mask=src_pad_mask)\n",
        "logits = final_proj(dec_out)  # [B, Lt, vocab]\n",
        "```\n",
        "> **Tip:** For autoâ€‘regressive *inference*, you typically loop tokenâ€‘byâ€‘token and only pass the prefix to the decoder, reâ€‘using cached keys/values for speed."
      ],
      "id": "mrlnraTc2Nnb"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qASwZhNz3k0S"
      },
      "id": "qASwZhNz3k0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) defining transformers in real-world situations"
      ],
      "metadata": {
        "id": "spoI3rC43-mi"
      },
      "id": "spoI3rC43-mi"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3,\n",
        "                 num_decoder_layers=3, dim_feedforward=512, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # (B, L, D)\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        B, Ls = src.shape\n",
        "        _, Lt = tgt.shape\n",
        "        device = src.device\n",
        "\n",
        "        # Positional indices\n",
        "        src_pos = torch.arange(Ls, device=device).unsqueeze(0).expand(B, Ls)\n",
        "        tgt_pos = torch.arange(Lt, device=device).unsqueeze(0).expand(B, Lt)\n",
        "\n",
        "        # Embedding + Position\n",
        "        src_emb = self.embedding(src) + self.pos_embedding(src_pos)\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_embedding(tgt_pos)\n",
        "\n",
        "        # Mask for auto-regressive decoding\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(Lt).to(device)\n",
        "\n",
        "        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "        logits = self.fc_out(out)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "UpNawgeE4CZQ"
      },
      "id": "UpNawgeE4CZQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}